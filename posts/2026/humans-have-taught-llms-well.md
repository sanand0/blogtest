---
title: Humans have taught LLMs well
date: 2026-01-08T14:29:56+08:00
categories:
  - llms
  - funny
---

![](https://files.s-anand.net/images/2026-01-08-humans-have-taught-llms-well.webp)

| Human                                                                                                                                                                               | LLM                                                                                                                                                                                                                             |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Bullshitting**: Humans confidently assert wrong information, from flat-earth beliefs to misremembered historical "facts" and fake news that spread through sheer conviction       | [**Hallucination**: LLMs generate plausible but factually incorrect content, stating falsehoods with the same fluency as facts](https://arxiv.org/abs/2311.05232)                                                               |
| **People-Pleasing**: Humans optimize for social harmony at the expense of honesty, nodding along with the boss's bad idea or validating a friend's flawed logic to avoid conflict   | [**Sycophancy**: LLMs trained with human feedback tell users what they want to hear, even confirming obviously wrong statements to avoid disagreement](https://arxiv.org/abs/2310.13548)                                        |
| **Zoning Out**: Humans lose focus during the middle of meetings, remembering the opening and closing but losing the substance sandwiched between                                    | [**Lost in the Middle**: LLMs perform well when key information appears at the start or end of input but miss crucial details positioned in the middle](https://arxiv.org/abs/2307.03172)                                       |
| **Overconfidence**: Humans often feel most certain precisely when they're least informed—a pattern psychologists have documented extensively in studies of overconfidence           | [**Poor Calibration**: LLMs express high confidence even when wrong, with stated certainty poorly correlated with actual accuracy](https://arxiv.org/abs/2306.13063)                                                            |
| **Trees for the Forest**: Humans can understand each step of a tax form yet still get the final number catastrophically wrong, failing to chain simple steps into complex inference | [**Compositional Reasoning Failure**: LLMs fail multi-hop reasoning tasks even when they can answer each component question individually](https://arxiv.org/abs/2402.14328)                                                     |
| **First Impressions**: Humans remember the first and last candidates interviewed while the middle blurs together, judging by position rather than merit                             | [**Position Bias**: LLMs systematically favor content based on position—preferring first or last items in lists regardless of quality](https://aclanthology.org/2024.findings-naacl.130/)                                       |
| **Tip-of-the-Tongue**: Humans can recite the alphabet forward but stumble backward, or remember the route to a destination but get lost returning                                   | [**Reversal Curse**: LLMs trained on "A is B" cannot infer "B is A"—knowing Tom Cruise's mother is Mary Lee Pfeiffer but failing to answer who her son is](https://arxiv.org/abs/2309.12288)                                    |
| **Framing Effects**: Humans give different answers depending on whether a procedure is framed as "90% survival rate" versus "10% mortality rate," despite identical meaning         | [**Prompt Sensitivity**: LLMs produce dramatically different outputs from minor, semantically irrelevant changes to prompt wording](https://arxiv.org/abs/2310.11324)                                                           |
| **Rambling**: Humans conflate length with thoroughness, trusting the thicker report and the longer meeting over concise alternatives                                                | [**Verbosity Bias**: LLMs produce unnecessarily verbose responses and, when evaluating text, systematically prefer longer outputs regardless of quality](https://arxiv.org/abs/2306.05685)                                      |
| **Armchair Expertise**: Humans hold forth on subjects they barely understand at dinner parties rather than simply saying "I don't know"                                             | [**Knowledge Boundary Blindness**: LLMs lack reliable awareness of what they know, generating confident fabrications rather than admitting ignorance](https://arxiv.org/abs/2305.18153)                                         |
| **Groupthink**: Humans pass down cognitive biases through culture and education, with students absorbing their teachers' bad habits                                                 | [**Bias Amplification**: LLMs exhibit amplified human cognitive biases including omission bias and framing effects, concentrating systematic errors from their training data](https://www.pnas.org/doi/10.1073/pnas.2412015122) |
| **Self-Serving Bias**: Humans rate their own work more generously than external judges would, finding their own prose clearer and arguments more compelling                         | [**Self-Enhancement Bias**: LLMs favor outputs from themselves or similar models when evaluating responses](https://arxiv.org/abs/2303.16634)                                                                                   |

Via [Claude](https://claude.ai/share/5998d509-aabf-479e-9ae0-464edc01ac46)

Inspired by [LLM problems observed in humans](https://embd.cc/llm-problems-observed-in-humans).
